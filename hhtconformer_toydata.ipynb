{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN4EwJiVSZsiU1fN6GTlAuN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wangbxj1234/eeg/blob/main/hhtconformer_toydata.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "LyRhwgjQEdbO"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "input: (N,C_in,Hin,Win)\n",
        "output:(N,C_out,Hout,Wout)\n",
        "\n",
        "Hout=[Hin+2×padding[0]−dilation[0]×(kernel_size[0]−1)−1]/stride[0]\n",
        "\n",
        "Wout=[Win+2×padding[1]−dilation[1]×(kernel_size[1]−1)−1]/stride[1]\n",
        "\n",
        "​\n",
        " \n"
      ],
      "metadata": {
        "id": "N3_-h-mmFvTc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "nn.Conv2d(C_in, C_out, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None)"
      ],
      "metadata": {
        "id": "se5QR6rfHr9D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#####vanilla shallownet ch=22\n",
        "shallownet = nn.Sequential(\n",
        "            nn.Conv2d(1, 40, (1, 25), (1, 1)),\n",
        "            nn.Conv2d(40, 40, (22, 1), (1, 1)),#torch.Size([144, 40, 1, 976])\n",
        "            nn.BatchNorm2d(40),\n",
        "            nn.ELU(),\n",
        "            nn.AvgPool2d((1, 75), (1, 15)),  # pooling acts as slicing to obtain 'patch' along the time dimension as in ViT\n",
        "            #torch.Size([144, 40, 1, 61])\n",
        "            nn.Dropout(0.5),\n",
        "        )\n",
        "\n",
        "input = torch.randn(144,1, 22, 1000)\n",
        "output = shallownet(input)\n",
        "output.size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IOwiSdwBqH-W",
        "outputId": "2bda65e6-4f92-4d77-b126-8dedb9ac4993"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([144, 40, 1, 61])"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.Patchembedding"
      ],
      "metadata": {
        "id": "Wors-XcM0v39"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "######with hht， ch=88\n",
        "shallownet = nn.Sequential(\n",
        "            nn.Conv2d(1, 40, (1, 25), (1, 1)),\n",
        "            nn.Conv2d(40, 40, (88, 1), (1, 1)),#torch.Size([144, 40, 1, 976])\n",
        "            nn.BatchNorm2d(40),\n",
        "            nn.ELU(),\n",
        "            nn.AvgPool2d((1, 75), (1, 15)),  # pooling acts as slicing to obtain 'patch' along the time dimension as in ViT\n",
        "            #torch.Size([144, 40, 1, 61])\n",
        "            nn.Dropout(0.5),\n",
        "        )\n",
        "\n",
        "input = torch.randn(144,1, 88, 1000)    ###toy data\n",
        "output = shallownet(input)\n",
        "output.size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u2vcHStAuSr3",
        "outputId": "c25f69f4-9b25-4fe1-be4d-9b3899e61207"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([144, 40, 1, 61])"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install einops"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R4-CPWc7x57G",
        "outputId": "4643a0d5-7ebd-4df1-92b9-0e31f900de9a"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.8/dist-packages (0.6.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import einops\n",
        "from einops import rearrange, reduce, repeat\n",
        "from einops.layers.torch import Rearrange, Reduce\n",
        "projection = nn.Sequential(\n",
        "            nn.Conv2d(40, 40, (1, 1), stride=(1, 1)),  # transpose, conv could enhance fiting ability slightly\n",
        "            Rearrange('b e (h) (w) -> b (h w) e'),\n",
        "        )\n",
        "x1 = projection(output)\n",
        "x1.size()#torch.Size([144, 61, 40])\n",
        "###finish the patchembedding"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fXi2DFaXxObZ",
        "outputId": "0a035dec-c02e-4322-82cd-369ef0df2444"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([144, 61, 40])"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.TransformerEncoder"
      ],
      "metadata": {
        "id": "TB5CUHDU0opm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import Tensor\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "EektUVs31B8B"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, emb_size, num_heads, dropout):\n",
        "        super().__init__()\n",
        "        self.emb_size = emb_size\n",
        "        self.num_heads = num_heads\n",
        "        self.keys = nn.Linear(emb_size, emb_size)\n",
        "        self.queries = nn.Linear(emb_size, emb_size)\n",
        "        self.values = nn.Linear(emb_size, emb_size)\n",
        "        self.att_drop = nn.Dropout(dropout)\n",
        "        self.projection = nn.Linear(emb_size, emb_size)\n",
        "\n",
        "    def forward(self, x: Tensor, mask: Tensor = None) -> Tensor:\n",
        "        queries = rearrange(self.queries(x), \"b n (h d) -> b h n d\", h=self.num_heads)\n",
        "        keys = rearrange(self.keys(x), \"b n (h d) -> b h n d\", h=self.num_heads)\n",
        "        values = rearrange(self.values(x), \"b n (h d) -> b h n d\", h=self.num_heads)\n",
        "        energy = torch.einsum('bhqd, bhkd -> bhqk', queries, keys)  \n",
        "        if mask is not None:\n",
        "            fill_value = torch.finfo(torch.float32).min\n",
        "            energy.mask_fill(~mask, fill_value)\n",
        "\n",
        "        scaling = self.emb_size ** (1 / 2)\n",
        "        att = F.softmax(energy / scaling, dim=-1)\n",
        "        att = self.att_drop(att)\n",
        "        #out = torch.einsum('bhal, bhlv -> bhav ', att, values)\n",
        "        out = torch.einsum('bhqk, bhkv -> bhkv ', att, values)\n",
        "        out = rearrange(out, \"b h n d -> b n (h d)\")\n",
        "        out = self.projection(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResidualAdd(nn.Module):\n",
        "    def __init__(self, fn):\n",
        "        super().__init__()\n",
        "        self.fn = fn\n",
        "\n",
        "    def forward(self, x, **kwargs):\n",
        "        res = x\n",
        "        x = self.fn(x, **kwargs)\n",
        "        x += res\n",
        "        return x\n",
        "\n",
        "\n",
        "class FeedForwardBlock(nn.Sequential):\n",
        "    def __init__(self, emb_size, expansion, drop_p):\n",
        "        super().__init__(\n",
        "            nn.Linear(emb_size, expansion * emb_size),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(drop_p),\n",
        "            nn.Linear(expansion * emb_size, emb_size),\n",
        "        )\n",
        "\n",
        "\n",
        "class GELU(nn.Module):\n",
        "    def forward(self, input: Tensor) -> Tensor:\n",
        "        return input*0.5*(1.0+torch.erf(input/math.sqrt(2.0)))\n",
        "\n",
        "\n",
        "class TransformerEncoderBlock(nn.Sequential):\n",
        "    def __init__(self,\n",
        "                 emb_size,\n",
        "                 num_heads=10,\n",
        "                 drop_p=0.5,\n",
        "                 forward_expansion=4,\n",
        "                 forward_drop_p=0.5):\n",
        "        super().__init__(\n",
        "            ResidualAdd(nn.Sequential(\n",
        "                nn.LayerNorm(emb_size),\n",
        "                MultiHeadAttention(emb_size, num_heads, drop_p),\n",
        "                nn.Dropout(drop_p)\n",
        "            )),\n",
        "            ResidualAdd(nn.Sequential(\n",
        "                nn.LayerNorm(emb_size),\n",
        "                FeedForwardBlock(\n",
        "                    emb_size, expansion=forward_expansion, drop_p=forward_drop_p),\n",
        "                nn.Dropout(drop_p)\n",
        "            )\n",
        "            ))\n",
        "\n",
        "\n",
        "class TransformerEncoder(nn.Sequential):\n",
        "    def __init__(self, depth, emb_size):\n",
        "        super().__init__(*[TransformerEncoderBlock(emb_size) for _ in range(depth)])"
      ],
      "metadata": {
        "id": "mZVghhL70QWq"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Encoder = TransformerEncoder(depth=1, emb_size=40)\n",
        "x2 =Encoder(x1)###TransformerEncoder finished\n",
        "x2.size()###torch.Size([144, 61, 40])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gTO__uHf04He",
        "outputId": "e1e02a4c-9771-459f-887b-0d029a1d34c6"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([144, 61, 40])"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.ClassificationHead"
      ],
      "metadata": {
        "id": "oZU9pJUE-WDy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ClassificationHead(nn.Sequential):\n",
        "    def __init__(self, emb_size, n_classes):\n",
        "        super().__init__()\n",
        "        \n",
        "        # global average pooling\n",
        "       # self.clshead = nn.Sequential(\n",
        "       #     Reduce('b n e -> b e', reduction='mean'),\n",
        "       #     nn.LayerNorm(emb_size),\n",
        "       #     nn.Linear(emb_size, n_classes)\n",
        "       # )\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(2440, 256),\n",
        "            nn.ELU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(256, 32),\n",
        "            nn.ELU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(32, 4)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.contiguous().view(x.size(0), -1)\n",
        "        out = self.fc(x)\n",
        "        return x, out  "
      ],
      "metadata": {
        "id": "1Tk1Uz8k-QJE"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Head = ClassificationHead(emb_size=40,n_classes=6)\n",
        "x3,out =Head(x2)###ClassificationHead finished\n",
        "x3.size()###torch.Size([144, 2440])\n",
        "out.size()###torch.Size([144, 4])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QtNtW2LR-Z24",
        "outputId": "87c25e7d-2b51-4954-dc07-83f06bc8007f"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([144, 4])"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xxje051OEcgy",
        "outputId": "a3521f7f-5720-4bcb-9221-d3a7e5ae9f22"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([20, 33, 24, 49])"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ],
      "source": [
        "# With square kernels and equal stride\n",
        "m = nn.Conv2d(16, 33, 3, stride=2)\n",
        "input = torch.randn(20, 16, 50, 100)\n",
        "output = m(input)\n",
        "output.size()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# non-square kernels and unequal stride and with padding\n",
        "m = nn.Conv2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2))\n",
        "input = torch.randn(20, 16, 50, 100)\n",
        "output = m(input)\n",
        "output.size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HrUJqsiMEmrZ",
        "outputId": "92a42468-4f4a-4fd7-da63-72a3d0b13f93"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([20, 33, 28, 100])"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# non-square kernels and unequal stride and with padding and dilation\n",
        "m = nn.Conv2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2), dilation=(3, 1))\n",
        "input = torch.randn(20, 16, 50, 100)\n",
        "output = m(input)\n",
        "output.size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4XuDZe43EwLG",
        "outputId": "a3ba9ddc-e555-44f1-e221-cb288a27d08b"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([20, 33, 26, 100])"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    }
  ]
}